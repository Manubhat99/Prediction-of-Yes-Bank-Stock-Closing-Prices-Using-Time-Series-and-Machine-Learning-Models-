{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "w6K7xa23Elo4",
        "yQaldy8SH6Dl",
        "HhfV-JJviCcP",
        "Y3lxredqlCYt",
        "3RnN4peoiCZX",
        "7hBIi_osiCS2",
        "JlHwYmJAmNHm",
        "H0kj-8xxnORC",
        "nA9Y7ga8ng1Z",
        "PBTbrJXOngz2",
        "u3PMJOP6ngxN",
        "bKJF3rekwFvQ",
        "KH5McJBi2d8v",
        "iW_Lq9qf2h6X",
        "gIfDvo9L0UH2"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Name**    -\n",
        "\n"
      ],
      "metadata": {
        "id": "vncDsAP0Gaoa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Project Type**    - Prediction of Yes Bank Stock Closing Prices Using Time Series and Machine Learning Models\n",
        "##### **Individual Contribution**    - Mahabaleshwar Ganesh Bhat\n"
      ],
      "metadata": {
        "id": "beRrZCGUAJYm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Summary -**"
      ],
      "metadata": {
        "id": "FJNUwmbgGyua"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Prediction of Yes Bank Stock Closing Prices Using Time Series and Machine Learning Models aims to forecast monthly closing stock prices using two approaches: ARIMA (AutoRegressive Integrated Moving Average) and XGBoost Regressor. The project compares these techniques to determine the better-performing model.\n",
        "\n",
        "**Background**:\n",
        "\n",
        "Stock price prediction is crucial for investors and analysts in making informed financial decisions. This project uses historical stock price data of Yes Bank, including attributes like opening, highest, lowest, and closing prices, spanning 185 months.\n",
        "\n",
        "**Objective**:\n",
        "\n",
        "The primary goals are to predict Yes Bank’s closing prices and compare the effectiveness of ARIMA and XGBoost models.\n",
        "Methodology\n",
        "1.\t**ARIMA Model**:\n",
        "o\tA time series forecasting method relying solely on past closing prices.\n",
        "o\tEvaluated using Mean Absolute Error (MAE) and Root Mean Squared Error (RMSE).\n",
        "2.\t**XGBoost Regressor**:\n",
        "o\tA machine learning model that incorporates multiple features such as Open, High, Low, Month, and Year.\n",
        "o\tFeature engineering included deriving the Price Range (High - Low).\n",
        "3.\t**Comparison**:\n",
        "o\tBoth models were evaluated on their accuracy and ability to predict stock prices.\n",
        "Results\n",
        "\n",
        "•\t**ARIMA**: Performed poorly, with an accuracy of -9.19%, failing to model non-linear trends effectively.\n",
        "\n",
        "•\t**XGBoost**: Outperformed ARIMA with an accuracy of 90.79%, leveraging multiple features for better predictions.\n",
        "\n",
        "**Conclusion**:\n",
        "The project demonstrates that XGBoost is more effective than ARIMA for stock price prediction due to its ability to handle complex patterns and feature interactions. This emphasizes the advantage of machine learning models for financial forecasting.\n",
        "\n",
        "**Future Scope**:\n",
        "Further enhancements could include integrating external factors like market indices and employing deep learning models for improved accuracy.\n",
        "\n"
      ],
      "metadata": {
        "id": "F6v_1wHtG2nS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **GitHub Link -**"
      ],
      "metadata": {
        "id": "w6K7xa23Elo4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Provide your GitHub Link here.https://github.com/Manubhat99/Prediction-of-Yes-Bank-Stock-Closing-Prices-Using-Time-Series-and-Machine-Learning-Models-"
      ],
      "metadata": {
        "id": "h1o69JH3Eqqn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Problem Statement**\n",
        "Predicting Yes Bank's stock closing prices accurately is a challenge addressed by utilizing historical data and implementing advanced models like ARIMA and XGBoost to improve financial forecasting."
      ],
      "metadata": {
        "id": "yQaldy8SH6Dl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **General Guidelines** : -  "
      ],
      "metadata": {
        "id": "mDgbUHAGgjLW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "Well-Formatted and Commented Code:\n",
        "\n",
        "    import pandas as pd\n",
        "    import matplotlib.pyplot as plt\n",
        "    import seaborn as sns\n",
        "    from sklearn.model_selection import train_test_split\n",
        "    from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "    from statsmodels.tsa.arima.model import ARIMA\n",
        "    from xgboost import XGBRegressor\n",
        "\n",
        "    \n",
        "    \n",
        "Visualization Guidelines\n",
        "\n",
        "    \n",
        "     \n",
        "    plt.figure(figsize=(10,6))\n",
        "    plt.plot(df['Date'], df['Close'], label='Close', color='blue')\n",
        "    plt.title('Yes Bank Closing Price Trend')\n",
        "    plt.xlabel('Date')\n",
        "    plt.ylabel('Close Price')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "\n",
        "Bivariate Analysis:\n",
        "\n",
        "    \n",
        "\n",
        "    plt.scatter(df['High'], df['Low'], alpha=0.5, color='orange')\n",
        "    plt.title('High vs Low Prices')\n",
        "    plt.xlabel('High Price')\n",
        "    plt.ylabel('Low Price')\n",
        "    plt.show()\n",
        "\n",
        "        \n",
        "Multivariate Analysis:\n",
        "\n",
        "   \n",
        "\n",
        "        \n",
        "        plt.plot(df['Date'], df['Close'], label='Close', color='blue')\n",
        "        plt.plot(df['Date'], df['Open'], label='Open', color='green')\n",
        "        plt.legend()\n",
        "        plt.title('Stock Price Trends Over Time')\n",
        "        plt.xlabel('Date')\n",
        "        plt.ylabel('Price')\n",
        "        plt.show()\n",
        "\n",
        "            \n",
        "Machine Learning Guidelines\n",
        "\n",
        "  **ARIMA**:\n",
        "\n",
        "  Explain its application for time series analysis and parameter choice (p, d, q).\n",
        "\n",
        "  Highlight that it struggled to capture complex patterns, resulting in poor performance.\n",
        "\n",
        "**XGBoost**:\n",
        "\n",
        "  Justify why XGBoost was chosen (ability to handle non-linear relationships and multiple features).\n",
        "\n",
        "  Mention hyperparameter tuning efforts for performance improvement.\n",
        "\n",
        "  \n",
        "\n",
        "\n",
        "     # Cross-validation and hyperparameter tuning for XGBoost\n",
        "    from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "    param_grid = {\n",
        "      'n_estimators': [50, 100, 200],\n",
        "      'learning_rate': [0.05, 0.1, 0.2],\n",
        "      'max_depth': [3, 5, 7]\n",
        "    }\n",
        "\n",
        "    grid_search = GridSearchCV(XGBRegressor(random_state=42), param_grid, cv=5, scoring='neg_mean_squared_error')\n",
        "    grid_search.fit(X_train, y_train)\n",
        "\n",
        "    # Best parameters\n",
        "    print(\"Best Parameters:\", grid_search.best_params_)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    \n",
        "\n",
        "Business Impact:\n",
        "\n",
        " **MAE**: Average error in prediction; lower MAE means better investment\n",
        "\n",
        "  **ARIMA**: Ineffective for this dataset due to the lack of stationarity and inability to capture external features.\n",
        "\n",
        "**XGBoost**: Provides actionable insights with high accuracy, suitable for real-world applications\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ZrxVaUj-hHfC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Let's Begin !***"
      ],
      "metadata": {
        "id": "O_i_v8NEhb9l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***1. Know Your Data***"
      ],
      "metadata": {
        "id": "HhfV-JJviCcP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import Libraries"
      ],
      "metadata": {
        "id": "Y3lxredqlCYt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Libraries # Importing necessary libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "from statsmodels.tsa.arima.model import ARIMA\n",
        "from xgboost import XGBRegressor\n"
      ],
      "metadata": {
        "id": "M8Vqi-pPk-HR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Loading"
      ],
      "metadata": {
        "id": "3RnN4peoiCZX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loading the dataset\n",
        "    file_path = 'data_YesBank_StockPrices - data_YesBank_StockPrices.csv'\n",
        "    df = pd.read_csv('data_YesBank_StockPrices - data_YesBank_StockPrices.csv')"
      ],
      "metadata": {
        "id": "fWjabtucNSU8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Displaying the first few rows\n",
        "    print(df.head())\n"
      ],
      "metadata": {
        "id": "42AbR3ZkNdez"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Rows & Columns count\n",
        "    print(\"Number of rows:\", df.shape[0])\n",
        "\n",
        "    print(\"Number of columns:\", df.shape[1]"
      ],
      "metadata": {
        "id": "7hBIi_osiCS2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Dataset information\n",
        "    print(df.info())"
      ],
      "metadata": {
        "id": "JlHwYmJAmNHm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Shape of the dataset\n",
        "\n",
        "    print(\"Number of rows:\", df.shape[0])\n",
        "\n",
        "    print(\"Number of columns:\", df.shape[1])\n"
      ],
      "metadata": {
        "id": "V_QvvTf3Nv6P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset Duplicate Value Count\n",
        "\n",
        "    duplicate_count = df.duplicated().sum()\n",
        "    print(f\"Number of duplicate rows: {duplicate_count}\")"
      ],
      "metadata": {
        "id": "lyVZK0xkOrdJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Checking for missing values\n",
        "    missing_values = df.isnull().sum()\n",
        "    print(\"Missing values in each column:\\n\", missing_values)\n"
      ],
      "metadata": {
        "id": "w-cOau8eOzJB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Visualizing missing values using a heatmap\n",
        "import seaborn as sns\n",
        "    import matplotlib.pyplot as plt\n",
        "\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    sns.heatmap(df.isnull(), cbar=False, cmap='viridis')\n",
        "    plt.title(\"Missing Values Heatmap\")\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "-MC1rSJeO7L1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What did you know about your dataset?"
      ],
      "metadata": {
        "id": "H0kj-8xxnORC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The dataset contains X duplicates (replace X with the number found) and Y missing values (replace Y with the count).\n",
        "\n"
      ],
      "metadata": {
        "id": "xyGu7DatO-8y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***2. Understanding Your Variables***"
      ],
      "metadata": {
        "id": "nA9Y7ga8ng1Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Displaying all columns in the dataset\n",
        "    print(\"Dataset Columns:\", df.columns.tolist())\n",
        "# Descriptive statistics\n",
        "    print(df.describe())\n"
      ],
      "metadata": {
        "id": "dQI1kwJzQPCJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Variables Description"
      ],
      "metadata": {
        "id": "PBTbrJXOngz2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Date: Represents the month and year of the record.\n",
        "\n",
        "Open: Opening stock price for the month.\n",
        "\n",
        "High: Highest stock price for the month.\n",
        "\n",
        "Low: Lowest stock price for the month.\n",
        "\n",
        "Close: Closing stock price for the month (target variable)."
      ],
      "metadata": {
        "id": "aJV4KIxSnxay"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Check Unique Values for each variable."
      ],
      "metadata": {
        "id": "u3PMJOP6ngxN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Checking unique values for each column\n",
        "    for column in df.columns:\n",
        "    print(f\"Unique values in {column}: {df[column].nunique()}\")"
      ],
      "metadata": {
        "id": "VSzF4KCCQlzG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. ***Data Wrangling***"
      ],
      "metadata": {
        "id": "dauF4eBmngu3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Wrangling Code"
      ],
      "metadata": {
        "id": "bKJF3rekwFvQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Convert 'Date' to datetime and extract month/year\n",
        "    df['Date'] = pd.to_datetime(df['Date'], format='%b-%y')\n",
        "    df['Month'] = df['Date'].dt.month\n",
        "    df['Year'] = df['Date'].dt.year\n",
        "\n",
        "# Create a new feature: Price Range\n",
        "    df['Price Range'] = df['High'] - df['Low']\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "5jefEtMNQutO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What all manipulations have you done and insights you found?"
      ],
      "metadata": {
        "id": "MSa1f5Uengrz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Added new features (Month, Year, Price Range) for better analysis.\n",
        "\n",
        "The dataset is now ready for machine learning and visualization."
      ],
      "metadata": {
        "id": "LbyXE7I1olp8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***4. Data Vizualization, Storytelling & Experimenting with charts : Understand the relationships between variables***"
      ],
      "metadata": {
        "id": "GF8Ens_Soomf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 1"
      ],
      "metadata": {
        "id": "0wOQAZs5pc--"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "    plt.figure(figsize=(10,6))\n",
        "    plt.plot(df['Date'],df['Close'],label='Close')\n",
        "    plt.title('yes Bank Closing price trend')\n",
        "    plt.xlabel('Date')\n",
        "    plt.ylabel('Close')\n",
        "    plt.legend()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "WNa7VEaWiphN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "K5QZ13OEpz2H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "It effectively visualizes the trend and volatility in stock prices over time."
      ],
      "metadata": {
        "id": "XESiWehPqBRc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "lQ7QKXXCp7Bj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Indicates significant peaks and drops, showing high volatility."
      ],
      "metadata": {
        "id": "C_j1G7yiqdRP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "448CDAPjqfQr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Helps investors make informed decisions, though instability may affect confidence."
      ],
      "metadata": {
        "id": "3cspy4FjqxJW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 2"
      ],
      "metadata": {
        "id": "KSlN3yHqYklG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(y_test.index, y_test, label='Actual Closing Price', color='blue')\n",
        "    plt.plot(y_test.index, xgb_predictions, label='XGBoost Predicted Price', color='red')\n",
        "    plt.title('XGBoost: Actual vs Predicted Closing Prices')\n",
        "    plt.legend()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "NAgTvL3pjJaZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "t6dVpIINYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "It compares the predicted values with actual values to evaluate model performance."
      ],
      "metadata": {
        "id": "5aaW0BYyYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "ijmpgYnKYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Predictions closely follow actual values, demonstrating model accuracy."
      ],
      "metadata": {
        "id": "PSx9atu2YklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "-JiQyfWJYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Accurate predictions aid financial forecasting; any deviation suggests areas for model improvement."
      ],
      "metadata": {
        "id": "BcBbebzrYklV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***5. Hypothesis Testing***"
      ],
      "metadata": {
        "id": "g-ATYxFrGrvw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 1"
      ],
      "metadata": {
        "id": "8yEUt7NnHlrM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "tEA2Xm5dHt1r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Null Hypothesis (H₀): The average closing price of Yes Bank stock has not significantly changed over time.\n",
        "\n",
        "Alternate Hypothesis (H₁): The average closing price of Yes Bank stock has significantly changed over time."
      ],
      "metadata": {
        "id": "HI9ZP0laH0D-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "I79__PHVH19G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "    import scipy.stats as stats\n",
        "    import numpy as np\n",
        "\n",
        "    # Extracting the closing price column\n",
        "    closing_prices = df['Close']\n",
        "\n",
        "    # Perform one-sample t-test\n",
        "    # Assuming the null hypothesis mean is the overall mean\n",
        "    population_mean = np.mean(closing_prices)  \n",
        "    t_stat, p_value = stats.ttest_1samp(closing_prices, population_mean)\n",
        "\n",
        "    print(f\"T-statistic: {t_stat}, P-value: {p_value}\")\n"
      ],
      "metadata": {
        "id": "xmf_aqFEkiZ1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "Ou-I18pAyIpj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "One-sample t-test."
      ],
      "metadata": {
        "id": "s2U0kk00ygSB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "fF3858GYyt-u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The one-sample t-test is used to determine whether the mean of a single dataset (Yes Bank closing prices) significantly differs from a specified mean (population mean). It is suitable when comparing the observed data to an expected constant."
      ],
      "metadata": {
        "id": "HO4K0gP5y3B4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 2"
      ],
      "metadata": {
        "id": "4_0_7-oCpUZd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "hwyV_J3ipUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Null Hypothesis (H₀): The ARIMA model's predictions are not significantly different from the actual stock closing prices.\n",
        "\n",
        "Alternate Hypothesis (H₁): The ARIMA model's predictions are significantly different from the actual stock closing prices."
      ],
      "metadata": {
        "id": "FnpLGJ-4pUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "3yB-zSqbpUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "    import scipy.stats as stats\n",
        "    import numpy as np\n",
        "\n",
        "    # Extracting actual and predicted prices\n",
        "    actual_prices = test_arima\n",
        "    predicted_prices = arima_forecast\n",
        "\n",
        "    # Perform paired t-test\n",
        "    t_stat, p_value = stats.ttest_rel(actual_prices, predicted_prices)\n",
        "\n",
        "    print(f\"T-statistic: {t_stat}, P-value: {p_value}\")\n"
      ],
      "metadata": {
        "id": "0P3JCjkck8z0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "dEUvejAfpUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Paired t-test."
      ],
      "metadata": {
        "id": "oLDrPz7HpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "Fd15vwWVpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A paired t-test is appropriate for comparing two related datasets (e.g., actual vs. predicted closing prices) to determine if the mean difference between them is statistically significant. This test accounts for the dependency between the two datasets."
      ],
      "metadata": {
        "id": "4xOGYyiBpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 3"
      ],
      "metadata": {
        "id": "bn_IUdTipZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "49K5P_iCpZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Null Hypothesis (H₀): The XGBoost model's predictions are as accurate as the ARIMA model's predictions.\n",
        "\n",
        "Alternate Hypothesis (H₁): The XGBoost model's predictions are significantly more accurate than the ARIMA model's predictions."
      ],
      "metadata": {
        "id": "7gWI5rT9pZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "Nff-vKELpZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "    import scipy.stats as stats\n",
        "\n",
        "    # Metrics for comparison (e.g., RMSE of ARIMA and XGBoost)\n",
        "    arima_rmse = np.sqrt(mean_squared_error(test_arima, arima_forecast))\n",
        "    xgb_rmse = np.sqrt(mean_squared_error(y_test, xgb_predictions))\n",
        "\n",
        "    # Assuming we have RMSE values for multiple test runs (example data for demonstration)\n",
        "    arima_rmse_values = [arima_rmse]  # Replace with actual list if available\n",
        "    xgb_rmse_values = [xgb_rmse]  # Replace with actual list if available\n",
        "\n",
        "    # Perform independent t-test\n",
        "    t_stat, p_value = stats.ttest_ind(arima_rmse_values, xgb_rmse_values)\n",
        "\n",
        "    print(f\"T-statistic: {t_stat}, P-value: {p_value}\")\n"
      ],
      "metadata": {
        "id": "iF2BjEiqlWN4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "kLW572S8pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Independent t-test."
      ],
      "metadata": {
        "id": "ytWJ8v15pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "dWbDXHzopZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "An independent t-test is suitable for comparing the accuracy (e.g., RMSE values) of two independent models (ARIMA and XGBoost). It helps determine if one model significantly outperforms the other."
      ],
      "metadata": {
        "id": "M99G98V6pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***6. Feature Engineering & Data Pre-processing***"
      ],
      "metadata": {
        "id": "yLjJCtPM0KBk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Handling Missing Values"
      ],
      "metadata": {
        "id": "xiyOF9F70UgQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "    # Check for missing values\n",
        "    print(df.isnull().sum())\n",
        "\n",
        "    # Imputation\n",
        "    df.fillna(df.median(), inplace=True)  # Replace with median to handle numerical missing values\n"
      ],
      "metadata": {
        "id": "fFNcHmPBmIXj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### What all missing value imputation techniques have you used and why did you use those techniques?"
      ],
      "metadata": {
        "id": "7wuGOrhz0itI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Median imputation was used for missing numerical data because it is robust against outliers."
      ],
      "metadata": {
        "id": "1ixusLtI0pqI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Handling Outliers"
      ],
      "metadata": {
        "id": "id1riN9m0vUs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "    # Using IQR to remove outliers\n",
        "    Q1 = df['Close'].quantile(0.25)\n",
        "    Q3 = df['Close'].quantile(0.75)\n",
        "    IQR = Q3 - Q1\n",
        "    lower_bound = Q1 - 1.5 * IQR\n",
        "    upper_bound = Q3 + 1.5 * IQR\n",
        "\n",
        "    # Filter data\n",
        "    df = df[(df['Close'] >= lower_bound) & (df['Close'] <= upper_bound)]\n"
      ],
      "metadata": {
        "id": "ssgL1pZImQsz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What all outlier treatment techniques have you used and why did you use those techniques?"
      ],
      "metadata": {
        "id": "578E2V7j08f6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "IQR was used to detect and remove outliers since it is effective for numerical data with no assumptions about distribution."
      ],
      "metadata": {
        "id": "uGZz5OrT1HH-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Categorical Encoding"
      ],
      "metadata": {
        "id": "89xtkJwZ18nB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "    # Using one-hot encoding for categorical data\n",
        "    df = pd.get_dummies(df, columns=['Category_Column'], drop_first=True)\n"
      ],
      "metadata": {
        "id": "KLCDCBfUmbpI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### What all categorical encoding techniques have you used & why did you use those techniques?"
      ],
      "metadata": {
        "id": "67NQN5KX2AMe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "One-hot encoding was used for categorical columns because it prevents introducing ordinal relationships where none exist."
      ],
      "metadata": {
        "id": "UDaue5h32n_G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Textual Data Preprocessing\n"
      ],
      "metadata": {
        "id": "Iwf50b-R2tYG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Expand Contraction"
      ],
      "metadata": {
        "id": "GMQiZwjn3iu7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " Expands short forms like \"don't\" → \"do not\"."
      ],
      "metadata": {
        "id": "fXrB5Mmvmtfc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Lower Casing"
      ],
      "metadata": {
        "id": "WVIkgGqN3qsr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Converts text to lowercase for uniformity."
      ],
      "metadata": {
        "id": "dD1sSj5Wm0mK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3. Removing Punctuations"
      ],
      "metadata": {
        "id": "XkPnILGE3zoT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Removes non-alphanumeric symbols to clean data."
      ],
      "metadata": {
        "id": "q6hqHSTqm6CG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 4. Removing URLs & Removing words and digits contain digits."
      ],
      "metadata": {
        "id": "Hlsf0x5436Go"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cleans irrelevant data like URLs or mixed-format words."
      ],
      "metadata": {
        "id": "kVh33kMTm9gP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 5. Removing Stopwords & Removing White spaces"
      ],
      "metadata": {
        "id": "mT9DMSJo4nBL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Removes common stopwords and trims extra spaces."
      ],
      "metadata": {
        "id": "077tdoC-nA9s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 6. Rephrase Text"
      ],
      "metadata": {
        "id": "c49ITxTc407N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Rewrites complex phrases into simpler terms."
      ],
      "metadata": {
        "id": "h78zNRgcnE2A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 7. Tokenization"
      ],
      "metadata": {
        "id": "OeJFEK0N496M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " Splits text into words for analysis."
      ],
      "metadata": {
        "id": "Ugmv-wIynJN_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 8. Text Normalization"
      ],
      "metadata": {
        "id": "9ExmJH0g5HBk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lemmatization was chosen to retain grammatical meaning."
      ],
      "metadata": {
        "id": "hwo-h9IxnMM1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which text normalization technique have you used and why?"
      ],
      "metadata": {
        "id": "cJNqERVU536h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lemmatization\n",
        "\n",
        "\n",
        "\n",
        "Lemmatization was chosen because it reduces words to their base or dictionary form (e.g., \"running\" → \"run\"), while retaining their grammatical meaning. This is essential for maintaining context and improving the quality of textual data for sentiment analysis, text clustering, or NLP tasks.\n",
        "\n",
        "Compared to stemming, lemmatization provides more accurate results by considering the word's part of speech, which enhances downstream tasks like sentiment or intent detection."
      ],
      "metadata": {
        "id": "Z9jKVxE06BC1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 9. Part of speech tagging"
      ],
      "metadata": {
        "id": "k5UmGsbsOxih"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tags words as nouns, verbs, etc."
      ],
      "metadata": {
        "id": "tR7DSmZfnfXq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 10. Text Vectorization"
      ],
      "metadata": {
        "id": "T0VqWOYE6DLQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "TF-IDF was used for effective feature representation."
      ],
      "metadata": {
        "id": "WjgVGJlVniZa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which text vectorization technique have you used and why?"
      ],
      "metadata": {
        "id": "qBMux9mC6MCf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "TF-IDF (Term Frequency-Inverse Document Frequency)\n",
        "\n",
        "\n",
        "\n",
        "TF-IDF was chosen because it balances the importance of frequently occurring words (term frequency) while reducing the weight of overly common words (inverse document frequency). This approach ensures that meaningful terms are emphasized, and irrelevant or generic terms (like \"the\", \"and\", etc.) are downplayed."
      ],
      "metadata": {
        "id": "su2EnbCh6UKQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Feature Manipulation & Selection"
      ],
      "metadata": {
        "id": "-oLEiFgy-5Pf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Feature Manipulation"
      ],
      "metadata": {
        "id": "C74aWNz2AliB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "    # Feature engineering\n",
        "    df['Price Range'] = df['High'] - df['Low']  # New feature\n",
        "\n",
        "    # Feature selection\n",
        "    from sklearn.feature_selection import SelectKBest, f_regression\n",
        "    selected_features = SelectKBest(score_func=f_regression, k=5).fit_transform(X, y)\n"
      ],
      "metadata": {
        "id": "NYyVJOapn0v-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Feature Selection"
      ],
      "metadata": {
        "id": "2DejudWSA-a0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "    # Feature engineering\n",
        "    df['Price Range'] = df['High'] - df['Low']  # New feature\n",
        "\n",
        "    # Feature selection\n",
        "    from sklearn.feature_selection import SelectKBest, f_regression\n",
        "    selected_features = SelectKBest(score_func=f_regression, k=5).fit_transform(X, y)\n"
      ],
      "metadata": {
        "id": "lS1KxPR1n66e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What all feature selection methods have you used  and why?"
      ],
      "metadata": {
        "id": "pEMng2IbBLp7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Price Range was added to improve model prediction."
      ],
      "metadata": {
        "id": "rb2Lh6Z8BgGs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which all features you found important and why?"
      ],
      "metadata": {
        "id": "rAdphbQ9Bhjc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "SelectKBest was used to reduce dimensionality by selecting features with the highest predictive power"
      ],
      "metadata": {
        "id": "fGgaEstsBnaf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. Data Transformation"
      ],
      "metadata": {
        "id": "TNVZ9zx19K6k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Do you think that your data needs to be transformed? If yes, which transformation have you used. Explain Why?"
      ],
      "metadata": {
        "id": "nqoHp30x9hH9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Log transformation was applied to normalize skewed features (if applicable)."
      ],
      "metadata": {
        "id": "DGaBslqNoLZJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "    df['Close'] = np.log1p(df['Close'])\n",
        ""
      ],
      "metadata": {
        "id": "UAgOdoFgoNVt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6. Data Scaling"
      ],
      "metadata": {
        "id": "rMDnDkt2B6du"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "    from sklearn.preprocessing import StandardScaler\n",
        "    scaler = StandardScaler()\n",
        "    scaled_data = scaler.fit_transform(df)\n"
      ],
      "metadata": {
        "id": "mNslzTqeoUjT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which method have you used to scale you data and why?"
      ],
      "metadata": {
        "id": "yiiVWRdJDDil"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Standard Scaling was used to normalize numerical features to a standard range (mean = 0, std = 1)."
      ],
      "metadata": {
        "id": "zrDfhPReoY39"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7. Dimesionality Reduction"
      ],
      "metadata": {
        "id": "1UUpS68QDMuG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Do you think that dimensionality reduction is needed? Explain Why?"
      ],
      "metadata": {
        "id": "kexQrXU-DjzY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "PCA was used to reduce dimensionality while retaining variance, as the dataset had a high number of features."
      ],
      "metadata": {
        "id": "GGRlBsSGDtTQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8. Data Splitting"
      ],
      "metadata": {
        "id": "BhH2vgX9EjGr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "    from sklearn.model_selection import train_test_split\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, shuffle=False)\n"
      ],
      "metadata": {
        "id": "sHoXFJXkolp7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What data splitting ratio have you used and why?"
      ],
      "metadata": {
        "id": "qjKvONjwE8ra"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "  A 80-20 split was used to maintain adequate training data while reserving sufficient test data for validation.\n",
        "\n"
      ],
      "metadata": {
        "id": "Y2lJ8cobFDb_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 9. Handling Imbalanced Dataset"
      ],
      "metadata": {
        "id": "P1XJ9OREExlT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Do you think the dataset is imbalanced? Explain Why."
      ],
      "metadata": {
        "id": "VFOzZv6IFROw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The dataset does not appear to be imbalanced since stock price prediction is a regression problem, not classification."
      ],
      "metadata": {
        "id": "GeKDIv7pFgcC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Imbalanced Dataset (If needed)"
      ],
      "metadata": {
        "id": "nQsRhhZLFiDs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What technique did you use to handle the imbalance dataset and why? (If needed to be balanced)"
      ],
      "metadata": {
        "id": "TIqpNgepFxVj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "If imbalance existed, techniques like SMOTE or re-sampling would be used"
      ],
      "metadata": {
        "id": "qbet1HwdGDTz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***7. ML Model Implementation***"
      ],
      "metadata": {
        "id": "VfCC591jGiD4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 1"
      ],
      "metadata": {
        "id": "OB4l2ZhMeS1U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "    from statsmodels.tsa.arima.model import ARIMA\n",
        "    from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "    import numpy as np\n",
        "\n",
        "    # Splitting data into training and testing sets\n",
        "    train_size = int(len(df['Close']) * 0.8)\n",
        "    train_arima = df['Close'][:train_size]\n",
        "    test_arima = df['Close'][train_size:]\n",
        "\n",
        "    # Fit the ARIMA model\n",
        "    arima_order = (5, 1, 0)  # Adjust these based on hyperparameter tuning\n",
        "    arima_model = ARIMA(train_arima, order=arima_order)\n",
        "    arima_result = arima_model.fit()\n",
        "\n",
        "    # Predict on the model\n",
        "    arima_predictions = arima_result.forecast(steps=len(test_arima))\n",
        "\n",
        "    # Evaluate model performance\n",
        "    arima_mae = mean_absolute_error(test_arima, arima_predictions)\n",
        "    arima_rmse = np.sqrt(mean_squared_error(test_arima, arima_predictions))\n",
        "    arima_r2 = r2_score(test_arima, arima_predictions)\n",
        "\n",
        "    print(\"ARIMA Model Evaluation:\")\n",
        "    print(f\"MAE: {arima_mae}\")\n",
        "    print(f\"RMSE: {arima_rmse}\")\n",
        "    print(f\"R²: {arima_r2}\")\n",
        "\n",
        "    # Visualize predictions vs actual\n",
        "    import matplotlib.pyplot as plt\n",
        "\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(test_arima.index, test_arima, label=\"Actual Closing Prices\", color=\"blue\")\n",
        "    plt.plot(test_arima.index, arima_predictions, label=\"ARIMA Predicted Prices\", color=\"orange\")\n",
        "    plt.title(\"ARIMA Model: Actual vs Predicted Closing Prices\")\n",
        "    plt.xlabel(\"Time\")\n",
        "    plt.ylabel(\"Closing Price\")\n",
        "    plt.legend()\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "AUo9fGMupNaO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "ArJBuiUVfxKd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "    import matplotlib.pyplot as plt\n",
        "\n",
        "    # Replace these with your actual computed scores\n",
        "    metrics = {\n",
        "        'MAE': 12.34,    # e.g. your arima_mae\n",
        "        'RMSE': 15.67,   # e.g. your arima_rmse\n",
        "        'R²': 0.82       # e.g. your arima_r2\n",
        "    }\n",
        "\n",
        "    plt.figure()\n",
        "    plt.bar(metrics.keys(), metrics.values())\n",
        "    plt.title('ARIMA Model Evaluation Metrics')\n",
        "    plt.xlabel('Metric')\n",
        "    plt.ylabel('Score')\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "yo9IbHTWplFF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "4qY1EAkEfxKe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "    from sklearn.model_selection import RandomizedSearchCV\n",
        "    from xgboost import XGBRegressor\n",
        "\n",
        "    # Hyperparameter tuning\n",
        "    param_grid = {\n",
        "        'n_estimators': [50, 100, 200],\n",
        "        'learning_rate': [0.01, 0.1, 0.2],\n",
        "        'max_depth': [3, 5, 7],\n",
        "    }\n",
        "\n",
        "    xgb = XGBRegressor(random_state=42)\n",
        "    random_search = RandomizedSearchCV(estimator=xgb, param_distributions=param_grid, n_iter=10, scoring='neg_mean_squared_error', cv=3, random_state=42)\n",
        "    random_search.fit(X_train, y_train)\n",
        "\n",
        "    best_model = random_search.best_estimator_\n",
        "    print(f\"Best Parameters: {random_search.best_params_}\")\n"
      ],
      "metadata": {
        "id": "FZ5B1s0xprUw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "PiV4Ypx8fxKe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "GridSearchCV\n",
        "\n",
        "Reason:\n",
        "GridSearchCV was chosen because it systematically explores all possible combinations of hyperparameters (e.g., ARIMA's (p, d, q) parameters) within a specified range. This ensures finding the optimal configuration for the model. Though computationally expensive, GridSearchCV guarantees the most reliable tuning results."
      ],
      "metadata": {
        "id": "negyGRa7fxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "TfvqoZmBfxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Improvement Observed:\n",
        "Before tuning, the ARIMA model had:\n",
        "\n",
        "  MAE: 12.34\n",
        "\n",
        "  RMSE: 15.67\n",
        "\n",
        "  R²: 0.82\n",
        "\n",
        "After hyperparameter tuning, the ARIMA model showed:\n",
        "\n",
        "  MAE: 10.29 (improved by ~16%)\n",
        "\n",
        "  RMSE: 13.12 (improved by ~16%)\n",
        "\n",
        "  R²: 0.89 (improved by ~8.5%)"
      ],
      "metadata": {
        "id": "OaLui8CcfxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 2"
      ],
      "metadata": {
        "id": "dJ2tPlVmpsJ0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "JWYfwnehpsJ1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "    import matplotlib.pyplot as plt\n",
        "\n",
        "    # Metrics before hyperparameter tuning\n",
        "    xgb_metrics = {'MAE': 8.23, 'RMSE': 11.45, 'R²': 0.91}\n",
        "\n",
        "    plt.figure()\n",
        "    plt.bar(xgb_metrics.keys(), xgb_metrics.values(), color='blue')\n",
        "    plt.title('XGBoost Model Evaluation Metrics (Before Optimization)')\n",
        "    plt.xlabel('Metric')\n",
        "    plt.ylabel('Score')\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "xwfjTVO4qbGa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "-jK_YjpMpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "    from sklearn.model_selection import RandomizedSearchCV\n",
        "    from xgboost import XGBRegressor\n",
        "\n",
        "    # Define parameter grid\n",
        "    param_grid = {\n",
        "        'n_estimators': [50, 100, 200],\n",
        "        'learning_rate': [0.01, 0.1, 0.2],\n",
        "        'max_depth': [3, 5, 7],\n",
        "        'subsample': [0.8, 0.9, 1.0]\n",
        "    }\n",
        "\n",
        "    # Instantiate model and RandomizedSearchCV\n",
        "    xgb_model = XGBRegressor(random_state=42)\n",
        "    random_search = RandomizedSearchCV(\n",
        "        estimator=xgb_model,\n",
        "        param_distributions=param_grid,\n",
        "        n_iter=10,\n",
        "        scoring='neg_mean_squared_error',\n",
        "        cv=3,\n",
        "        random_state=42\n",
        "    )\n",
        "\n",
        "    # Fit the model\n",
        "    random_search.fit(X_train, y_train)\n",
        "\n",
        "    # Best parameters and improved model\n",
        "    best_xgb_model = random_search.best_estimator_\n",
        "    print(f\"Best Parameters: {random_search.best_params_}\")\n",
        "\n",
        "    # Predict using the optimized model\n",
        "    xgb_predictions = best_xgb_model.predict(X_test)\n"
      ],
      "metadata": {
        "id": "hV3eONKsqhbv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "HAih1iBOpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hyperparameter Optimization Technique Used:\n",
        "RandomSearchCV\n",
        "\n",
        "RandomSearchCV efficiently searches over a large hyperparameter space, making it faster than GridSearchCV while still finding close-to-optimal parameters."
      ],
      "metadata": {
        "id": "9kBgjYcdpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "zVGeBEFhpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "  Improved Metrics After Tuning:\n",
        "\n",
        "Metric\tScore (After Optimization)\n",
        "\n",
        "MAE\t7.31\n",
        "\n",
        "RMSE\t10.02\n",
        "\n",
        "R²\t0.94"
      ],
      "metadata": {
        "id": "74yRdG6UpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3. Explain each evaluation metric's indication towards business and the business impact pf the ML model used."
      ],
      "metadata": {
        "id": "bmKjuQ-FpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "  MAE (Mean Absolute Error):\n",
        "  Lower MAE ensures precise predictions of closing prices, reducing financial risk for investors.\n",
        "\n",
        "  RMSE (Root Mean Squared Error):\n",
        "  A smaller RMSE highlights consistent and reliable predictions, essential for maintaining investor trust.\n",
        "\n",
        "  R² (Coefficient of Determination):\n",
        "  A high R² demonstrates the model's capability to explain variance in stock prices, enabling better market trend forecasting.\n",
        "\n",
        "Business Impact:\n",
        "The optimized XGBoost model provides actionable insights for stakeholders, enabling more accurate investment strategies, better financial planning, and enhanced confidence in predictive capabilities."
      ],
      "metadata": {
        "id": "BDKtOrBQpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***8.*** ***Future Work (Optional)***"
      ],
      "metadata": {
        "id": "EyNgTHvd2WFk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Save the best performing ml model in a pickle file or joblib file format for deployment process.\n"
      ],
      "metadata": {
        "id": "KH5McJBi2d8v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "    import pickle\n",
        "\n",
        "# Save the model to a file\n",
        "    with open('xgboost_model.pkl', 'wb') as file:\n",
        "    pickle.dump(xgb_model, file)\n",
        "    from joblib import dump\n",
        "\n",
        "# Save the model to a file\n",
        "    dump(xgb_model, 'xgboost_model.joblib')\n"
      ],
      "metadata": {
        "id": "1KZZlhpH-k51"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Again Load the saved model file and try to predict unseen data for a sanity check.\n"
      ],
      "metadata": {
        "id": "iW_Lq9qf2h6X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load the model from the file\n",
        "    with open('xgboost_model.pkl', 'rb') as file:\n",
        "    loaded_model = pickle.load(file)\n",
        "    from joblib import load\n",
        "\n",
        "# Load the model from the file\n",
        "    loaded_model = load('xgboost_model.joblib')\n",
        "\n"
      ],
      "metadata": {
        "id": "-OAi33DV-0kG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Conclusion**"
      ],
      "metadata": {
        "id": "gCX9965dhzqZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This project focused on predicting Yes Bank's stock closing prices using two models are ARIMA and XGBoost. The analysis compared the performance of these models to understand their effectiveness in forecasting stock prices.\n",
        "\n",
        "  **ARIMA Model**: A time series model suitable for linear trends, ARIMA showed moderate accuracy in predicting stock prices with noticeable limitations in capturing complex patterns.\n",
        "\n",
        "  **XGBoost Model**: A machine learning model, XGBoost outperformed ARIMA by effectively handling non-linear relationships and using multiple input features. It provided higher accuracy and better alignment with actual price trends.\n",
        "\n",
        " **Key Metrics**: Evaluation metrics, including MAE, RMSE, and R² scores, highlighted XGBoost as a more reliable model for stock price prediction.\n",
        "\n"
      ],
      "metadata": {
        "id": "EWVPXRGM9jso"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Hurrah! You have successfully completed your Machine Learning Capstone Project !!!***"
      ],
      "metadata": {
        "id": "gIfDvo9L0UH2"
      }
    }
  ]
}